{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2480a20f",
   "metadata": {},
   "source": [
    "# Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fa425b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\etudiant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\etudiant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "# Télécharger les ressources nécessaires pour NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "# Préparation des outils\n",
    "stop_words = set(stopwords.words('french'))\n",
    "stemmer = FrenchStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891c7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb09ad25",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f18c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>name</th>\n",
       "      <th>appreciation</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'$oid': '647bab3240fabfafb506263d'}</td>\n",
       "      <td>Dahl de lentilles corail</td>\n",
       "      <td>4.5/5</td>\n",
       "      <td>[Recette de base que j'adapte en fonction des ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'$oid': '647bab3240fabfafb506263e'}</td>\n",
       "      <td>Poêlée de panais</td>\n",
       "      <td>4.4/5</td>\n",
       "      <td>[Délicieux. C'est la première fois que je cuis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'$oid': '647bab3240fabfafb506263f'}</td>\n",
       "      <td>Aubergines au four</td>\n",
       "      <td>4.3/5</td>\n",
       "      <td>[Facile à préparer et très sain. Il faut mettr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'$oid': '647bab3240fabfafb5062640'}</td>\n",
       "      <td>Curry de pois chiches</td>\n",
       "      <td>4.8/5</td>\n",
       "      <td>[après 10' j'ajoute des épinards., Rien à ajou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'$oid': '647bab3240fabfafb5062641'}</td>\n",
       "      <td>Galettes de pomme de terre</td>\n",
       "      <td>3.9/5</td>\n",
       "      <td>[Très bien. Je n'ai mis qu'un oignon que j'ai ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    _id                        name  \\\n",
       "0  {'$oid': '647bab3240fabfafb506263d'}    Dahl de lentilles corail   \n",
       "1  {'$oid': '647bab3240fabfafb506263e'}            Poêlée de panais   \n",
       "2  {'$oid': '647bab3240fabfafb506263f'}          Aubergines au four   \n",
       "3  {'$oid': '647bab3240fabfafb5062640'}       Curry de pois chiches   \n",
       "4  {'$oid': '647bab3240fabfafb5062641'}  Galettes de pomme de terre   \n",
       "\n",
       "  appreciation                                           comments  \n",
       "0        4.5/5  [Recette de base que j'adapte en fonction des ...  \n",
       "1        4.4/5  [Délicieux. C'est la première fois que je cuis...  \n",
       "2        4.3/5  [Facile à préparer et très sain. Il faut mettr...  \n",
       "3        4.8/5  [après 10' j'ajoute des épinards., Rien à ajou...  \n",
       "4        3.9/5  [Très bien. Je n'ai mis qu'un oignon que j'ai ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('comments_f1.json')\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadee19b",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada7988",
   "metadata": {},
   "source": [
    "## 1- Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b4098",
   "metadata": {},
   "source": [
    "### Detecter les éléments à nettoyer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5582ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Présence dans les commentaires :\n",
      "Urls: 4 occurrences\n",
      "Mentions: 0 occurrences\n",
      "Uppercase words: 2063 occurrences\n",
      "Short words: 21678 occurrences\n",
      "Emojis: 320 occurrences\n",
      "Special characters: 22202 occurrences\n",
      "Doublons présents : Oui\n"
     ]
    }
   ],
   "source": [
    "def has_url(comment):\n",
    "    return bool(re.search(r'http[s]?://\\S+', comment))\n",
    "\n",
    "def has_mention(comment):\n",
    "    return bool(re.search(r'@\\w+', comment))\n",
    "\n",
    "def has_uppercase_words(comment):\n",
    "    return any(word.isupper() for word in comment.split())\n",
    "\n",
    "def has_short_words(comment, threshold=3):\n",
    "    return any(len(word) < threshold for word in comment.split())\n",
    "\n",
    "def has_emojis(comment):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return bool(emoji_pattern.search(comment))\n",
    "\n",
    "def has_special_characters(comment):\n",
    "    return bool(re.search(r'[^\\w\\s]', comment))\n",
    "\n",
    "def has_duplicates(comments):\n",
    "    seen = set()\n",
    "    for comment in comments:\n",
    "        if comment in seen:\n",
    "            return True  # Doublon trouvé\n",
    "        seen.add(comment)\n",
    "    return False  # Aucun doublon\n",
    "\n",
    "# Charger les données JSON\n",
    "comments = data['comments'].explode().tolist()\n",
    "\n",
    "# Statistiques de présence\n",
    "stats = {\n",
    "    \"urls\": 0,\n",
    "    \"mentions\": 0,\n",
    "    \"uppercase_words\": 0,\n",
    "    \"short_words\": 0,\n",
    "    \"emojis\": 0,\n",
    "    \"special_characters\": 0,\n",
    "    \"duplicates\": \"Non\"  # Initialise avec \"Non\"\n",
    "}\n",
    "\n",
    "# Vérification de la présence des éléments dans chaque commentaire\n",
    "for comment in comments:\n",
    "    if has_url(comment):\n",
    "        stats['urls'] += 1\n",
    "    if has_mention(comment):\n",
    "        stats['mentions'] += 1\n",
    "    if has_uppercase_words(comment):\n",
    "        stats['uppercase_words'] += 1\n",
    "    if has_short_words(comment):\n",
    "        stats['short_words'] += 1\n",
    "    if has_emojis(comment):\n",
    "        stats['emojis'] += 1\n",
    "    if has_special_characters(comment):\n",
    "        stats['special_characters'] += 1\n",
    "\n",
    "# Vérification des doublons\n",
    "stats['duplicates'] = \"Oui\" if has_duplicates(comments) else \"Non\"\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Présence dans les commentaires :\")\n",
    "for key, value in stats.items():\n",
    "    if key == \"duplicates\":\n",
    "        print(f\"Doublons présents : {value}\")\n",
    "    else:\n",
    "        print(f\"{key.capitalize().replace('_', ' ')}: {value} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d505a1ad",
   "metadata": {},
   "source": [
    "### Supprimer les problèmes detectés "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8409137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    cleaned_comments\n",
      "0  Recette de base que j'adapte en fonction des l...\n",
      "1  Noubliez pas de faire cuire en avance les caro...\n",
      "2  D'accord pour le commentaire sur les carottes ...\n",
      "3  Très bonne recette réalisée avec des lentilles...\n",
      "4              Facile à faire plat délicieux et sain\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(comment):\n",
    "    \"\"\"Supprime les URLs d'un commentaire.\"\"\"\n",
    "    return re.sub(r'http[s]?://\\S+', '', comment)\n",
    "\n",
    "def remove_emojis(comment):\n",
    "    \"\"\"Supprime tous les emojis d'un commentaire.\"\"\"\n",
    "    return emoji.replace_emoji(comment, replace='')\n",
    "\n",
    "def clean_punctuation(sentence):\n",
    "    \"\"\" Remplace certaines formes de ponctuation par des espaces et supprime d'autres, tout en préservant les apostrophes dans les contractions. \"\"\"\n",
    "    # Suppression de la ponctuation sauf les apostrophes utilisées dans les contractions\n",
    "    sentence = re.sub(r'(?<!\\w)[\\'\\\"](?!\\w)', '', sentence)  # Supprime les apostrophes et guillemets qui ne sont pas entourés par des lettres\n",
    "    sentence = re.sub(r'[?|!|:|;|.|,|)|(|\\|/]', ' ', sentence)  # Remplace d'autres ponctuations par des espaces\n",
    "    sentence = sentence.replace(\"\\n\", \" \")\n",
    "    return sentence.strip()\n",
    "\n",
    "def keep_alpha(sentence):\n",
    "    \"\"\"Conserve uniquement les caractères alphabétiques et gère certains caractères spéciaux.\"\"\"\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-ZÀ-ÿ\\']+', '', word)  # Inclure les caractères accentués français et apostrophes\n",
    "        alpha_word = alpha_word.replace('œ', 'oe')  # Remplacer 'œ' par 'oe'\n",
    "        alpha_sent += alpha_word + \" \"\n",
    "    return alpha_sent.strip()\n",
    "\n",
    "def remove_repeated_characters(comment):\n",
    "    \"\"\"Réduit les caractères répétés dans les mots pour éviter des variations comme 'délicieeuuuuux' à 'délicieux'.\"\"\"\n",
    "    return re.sub(r'(\\w)(\\1{2,})', r'\\1', comment)\n",
    "\n",
    "# Chargement et préparation des données\n",
    "comments = data['comments'].explode().drop_duplicates().tolist()\n",
    "\n",
    "# Application du nettoyage\n",
    "cleaned_comments = []\n",
    "for comment in comments:\n",
    "    comment = remove_urls(comment)\n",
    "    comment = remove_emojis(comment)\n",
    "    comment = clean_punctuation(comment)\n",
    "    comment = keep_alpha(comment)\n",
    "    comment = remove_repeated_characters(comment)\n",
    "    cleaned_comments.append(comment)\n",
    "    \n",
    "# Création d'un DataFrame à partir des commentaires nettoyés\n",
    "cleaned_data = pd.DataFrame(cleaned_comments, columns=['cleaned_comments'])\n",
    "\n",
    "# Afficher les premiers éléments nettoyés pour vérifier\n",
    "print(cleaned_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5afb4c9",
   "metadata": {},
   "source": [
    "## 2- Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2be46874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    \"\"\"Tokenise le texte en mots.\"\"\"\n",
    "    return word_tokenize(text, language='french')\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    \"\"\"Enlève les stop words de la liste de tokens.\"\"\"\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Applique le stemming sur la liste de mots.\"\"\"\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def labelize_scores(score):\n",
    "    \"\"\"Convertit un score textuel en une étiquette numérique simple.\"\"\"\n",
    "    return int(round(float(score.split('/')[0])))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Applique le tokenization, la suppression des stop words, et le stemming.\"\"\"\n",
    "    tokens = tokenize_sentences(text)\n",
    "    filtered_tokens = remove_stop_words(tokens)\n",
    "    stemmed_tokens = stem_words(filtered_tokens)\n",
    "    return stemmed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb1d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "408e731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               preprocessed_comments\n",
      "0  recet bas j'adapt fonction légum maison c'est ...\n",
      "1  noubl fair cuir avanc carott sinon recet tres ...\n",
      "2  d'accord commentair carott je n'ai trouv ça re...\n",
      "3  tres bon recet réalis lentill vert davantag la...\n",
      "4                        facil fair plat délici sain\n",
      "     appreciation\n",
      "0               4\n",
      "1               4\n",
      "2               4\n",
      "3               5\n",
      "4               4\n",
      "..            ...\n",
      "360             4\n",
      "361             5\n",
      "362             4\n",
      "363             4\n",
      "364             5\n",
      "\n",
      "[365 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données nettoyées\n",
    "cleaned_comments = cleaned_data['cleaned_comments'].explode().drop_duplicates().tolist()\n",
    "\n",
    "# Application du prétraitement\n",
    "preprocessed_comments = []\n",
    "for comment in cleaned_comments:\n",
    "    processed_text = preprocess_text(comment)\n",
    "    # Joindre les tokens pour former une chaîne de caractères avant de les ajouter à la liste\n",
    "    preprocessed_comments.append(' '.join(processed_text))\n",
    "\n",
    "# Création d'un DataFrame à partir des commentaires prétraités\n",
    "preprocessed_data = pd.DataFrame(preprocessed_comments, columns=['preprocessed_comments'])\n",
    "\n",
    "# Application de la fonction labelize_scores à la colonne des scores\n",
    "data['appreciation'] = data['appreciation'].apply(labelize_scores) \n",
    "\n",
    "# Appliquer la transformation directement sur la colonne 'appreciation'\n",
    "data['appreciation'] = data['appreciation'].apply(lambda x: int(round(float(x.split('/')[0]))))\n",
    "\n",
    "\n",
    "# Affichage des premiers éléments prétraités pour vérifier\n",
    "print(preprocessed_data.head())\n",
    "\n",
    "# Affichage des scores transformés pour vérifier\n",
    "print(data[['appreciation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e994558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>appreciation</th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dahl de lentilles corail</td>\n",
       "      <td>4</td>\n",
       "      <td>[Recette de base que j'adapte en fonction des ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Poêlée de panais</td>\n",
       "      <td>4</td>\n",
       "      <td>[Délicieux. C'est la première fois que je cuis...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aubergines au four</td>\n",
       "      <td>4</td>\n",
       "      <td>[Facile à préparer et très sain. Il faut mettr...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Curry de pois chiches</td>\n",
       "      <td>5</td>\n",
       "      <td>[après 10' j'ajoute des épinards., Rien à ajou...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Galettes de pomme de terre</td>\n",
       "      <td>4</td>\n",
       "      <td>[Très bien. Je n'ai mis qu'un oignon que j'ai ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  appreciation  \\\n",
       "0    Dahl de lentilles corail             4   \n",
       "1            Poêlée de panais             4   \n",
       "2          Aubergines au four             4   \n",
       "3       Curry de pois chiches             5   \n",
       "4  Galettes de pomme de terre             4   \n",
       "\n",
       "                                            comments  label  \n",
       "0  [Recette de base que j'adapte en fonction des ...      4  \n",
       "1  [Délicieux. C'est la première fois que je cuis...      4  \n",
       "2  [Facile à préparer et très sain. Il faut mettr...      4  \n",
       "3  [après 10' j'ajoute des épinards., Rien à ajou...      5  \n",
       "4  [Très bien. Je n'ai mis qu'un oignon que j'ai ...      4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if '_id' in data.columns:\n",
    "    data.drop('_id', axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "776cb609",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json(\"Cleaned_data.json\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9f2f66e",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 6808)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22316\\3243310695.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cleaned_data.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdata_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m     return loads(fp.read(),\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Extra data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 6808)"
     ]
    }
   ],
   "source": [
    "with open('Cleaned_data.json', 'r', encoding='utf-8') as f:\n",
    "    data_loaded = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fca556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
